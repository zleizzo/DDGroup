{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "dir_data = '../data/regr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization-based approach\n",
    "Let R denote (a parameterization of) our estimate for the region. The idea is to maximize $$\\textrm{vol}(R) - \\lambda \\sum_{i=1}^n f_i(R),$$ where $f_i(R)$ are functions which penalize R for including rejected points and potentially reward R for including non-rejected points.\n",
    "\n",
    "Note: We may want to include a normalized volume in the objective. When the region is small, the gradient of the volume will be close to vanishing, but when the region is large, the gradient of the volume will be huge.\n",
    "\n",
    "## Hard-thresholding method\n",
    "We first hard-threshold each training point based on a residual cutoff on the core fit, giving a binary label $r_i \\in \\{0,1\\}$ which is 1 if the i-th point is rejected and 0 if it is not. We then define $$f_i(R) = r_i \\exp(-c_1 d(x_i, R)) - (1-r_i) \\exp(-c_2 d(x_i, R)).$$\n",
    "Here $c_i$ are constants which control how much a rejected point is penalized (resp. a non-rejected point is rewarded) for being close to the approximate region. Note that for this particular objective, once a point is included in R, it stops contributing to the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Here we load the full dataset, determine the train/test split, and also find the \"core\" group. We also preprocess to add a bias term to the features and center the features so they have mean 0. Lastly, we find the bounding box for the data, which will be used as an \"outer bound\" for each method.\n",
    "\n",
    "The names of the options and their (n, d) values are:\n",
    "\n",
    "- Dutch_drinking_inh (12121, 16)\n",
    "- Dutch_drinking_wm (12131, 16)\n",
    "- Dutch_drinking_sha (12098, 16)\n",
    "- Brazil_health_heart (7728, 6)\n",
    "- Brazil_health_stroke (9675, 6)\n",
    "- Korea_grip (1022, 11)\n",
    "- China_glucose_women2 (4568, 11)\n",
    "- China_glucose_men2 (4360, 11)\n",
    "- Spain_Hair (529, 5)\n",
    "- China_HIV (2410, 27)\n",
    "\n",
    "We also include a 'Synthetic' option. If this is selected, then the dir_data argument should be replaced with [n, d] specifying the number of data points to be generated and the desired dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Synthetic'\n",
    "dir_data = [10000, 4]\n",
    "\n",
    "# dataset = 'Brazil_health_stroke'\n",
    "\n",
    "\n",
    "if dataset == 'Synthetic':\n",
    "    n, d = dir_data\n",
    "    w    = np.ones(d)\n",
    "    bias = 10.\n",
    "    std  = 0.3\n",
    "\n",
    "    R = np.ones((d, 2)) * (3 ** (-1/d))\n",
    "    R[:, 0] *= -1\n",
    "\n",
    "    X = 2 * np.random.rand(n, d) - 1\n",
    "    Y = std * np.random.randn(n)\n",
    "    for i in range(n):\n",
    "        if in_box(X[i], R):\n",
    "            Y[i] += np.dot(X[i], w) + bias\n",
    "\n",
    "    X_core = (6 ** (-1/d)) * (2 * np.random.rand(int(n / 10), d) - 1)\n",
    "    Y_core = X_core @ w + bias + np.random.randn(int(n / 10))\n",
    "    names_covariates = [X_core, Y_core, R]\n",
    "\n",
    "    B = np.ones((d, 2))\n",
    "    B[:, 0] *= -1\n",
    "\n",
    "else:\n",
    "    X, Y, names_covariates = load_regr_data(dataset)\n",
    "\n",
    "    n = len(Y)\n",
    "    d = len(X[0])\n",
    "\n",
    "    X -= np.mean(X, axis = 0) # We're cheating a little bit here because we also center the test data; easy to fix\n",
    "    B = np.column_stack([np.min(X, axis = 0), np.max(X, axis = 0)])\n",
    "\n",
    "X = np.concatenate([X, np.ones((n, 1))], axis = 1)\n",
    "\n",
    "test_ind = np.random.choice(range(n), size = int(n / 10), replace = False)\n",
    "train_ind = [i for i in range(n) if i not in test_ind]\n",
    "\n",
    "X_test = X[test_ind].copy()\n",
    "Y_test = Y[test_ind].copy()\n",
    "\n",
    "X_train = X[train_ind].copy()\n",
    "Y_train = Y[train_ind].copy()\n",
    "\n",
    "assert len(Y_test) + len(Y_train) == n\n",
    "\n",
    "if dataset == 'Synthetic':\n",
    "    X_core, Y_core, true_R = names_covariates\n",
    "    X_core = np.concatenate([X_core, np.ones((len(X_core), 1))], axis = 1)\n",
    "else:\n",
    "    X_core, Y_core = find_core(X_train, Y_train)\n",
    "\n",
    "beta, min_eig, s_hat = core_fit(X_core, Y_core)\n",
    "n_core = len(Y_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute point inclusion labels\n",
    "Using the core fit, we determine the inclusion/exclusion labels for each point. These can be 0/1 labels for hard cutoff methods, or soft labels or log p values (these latter two have not been implemented yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "hg_labels = hard_grow_labels(X, Y, alpha, s_hat, min_eig, n_core, beta)\n",
    "ho_labels = hard_opt_labels(X, Y, alpha, s_hat, min_eig, n_core, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute good region\n",
    "Using the labels defined above, approximate the \"good\" region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1000,  0.1000],\n",
      "        [-0.1000,  0.1000],\n",
      "        [-0.1000,  0.1000],\n",
      "        [-0.1000,  0.1000]], requires_grad=True)\n",
      "tensor([[-0.1853,  0.1842],\n",
      "        [-0.1863,  0.1839],\n",
      "        [-0.1884,  0.1862],\n",
      "        [-0.1872,  0.1855]], requires_grad=True)\n",
      "tensor([[-0.2669,  0.2654],\n",
      "        [-0.2693,  0.2646],\n",
      "        [-0.2737,  0.2690],\n",
      "        [-0.2715,  0.2679]], requires_grad=True)\n",
      "tensor([[-0.3444,  0.3427],\n",
      "        [-0.3482,  0.3411],\n",
      "        [-0.3551,  0.3477],\n",
      "        [-0.3518,  0.3465]], requires_grad=True)\n",
      "tensor([[-0.4174,  0.4157],\n",
      "        [-0.4228,  0.4130],\n",
      "        [-0.4317,  0.4214],\n",
      "        [-0.4272,  0.4205]], requires_grad=True)\n",
      "tensor([[-0.4843,  0.4835],\n",
      "        [-0.4923,  0.4785],\n",
      "        [-0.5023,  0.4899],\n",
      "        [-0.4970,  0.4890]], requires_grad=True)\n",
      "tensor([[-0.5433,  0.5453],\n",
      "        [-0.5554,  0.5365],\n",
      "        [-0.5654,  0.5526],\n",
      "        [-0.5591,  0.5511]], requires_grad=True)\n",
      "tensor([[-0.5950,  0.5999],\n",
      "        [-0.6104,  0.5869],\n",
      "        [-0.6197,  0.6078],\n",
      "        [-0.6149,  0.6047]], requires_grad=True)\n",
      "tensor([[-0.6398,  0.6472],\n",
      "        [-0.6571,  0.6294],\n",
      "        [-0.6662,  0.6540],\n",
      "        [-0.6622,  0.6508]], requires_grad=True)\n",
      "tensor([[-0.6774,  0.6864],\n",
      "        [-0.6972,  0.6655],\n",
      "        [-0.7048,  0.6923],\n",
      "        [-0.7012,  0.6902]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "hg_args = [None]\n",
    "\n",
    "init_R = 0.1 * torch.ones(d, 2)\n",
    "init_R[:, 0] *= -1\n",
    "\n",
    "reg   = 10. / n\n",
    "iters = 100\n",
    "lr    = 0.01\n",
    "c1    = 1.\n",
    "c2    = 1.\n",
    "\n",
    "ho_args = [init_R, reg, iters, lr, c1, c2]\n",
    "\n",
    "\n",
    "hg_R = hard_grow_region(X[:, :-1], hg_labels, B, hg_args)\n",
    "ho_R = hard_opt_region(torch.tensor(X[:, :-1]), torch.tensor(ho_labels), torch.tensor(B), ho_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results\n",
    "Check how good the region selected by each method is. For all datasets, we can look at the MAE of the model on the selected region. For the synthetic case, we know the \"correct\" region to select, and we can compute precision and recall for the region itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "Baseline MAE: 4.434643256347318\n",
      "---------------------------------\n",
      "Hard thresholding + growing box:\n",
      "MAE: 0.23709609549318472\n",
      "Incl. frac: 0.325\n",
      "Region precision: 0.9972650786138773\n",
      "Region recall: 1.0\n",
      "---------------------------------\n",
      "Hard thresholding + optimization:\n",
      "MAE: 0.2370989354458773\n",
      "Incl. frac: 0.261\n",
      "Region precision: 1.0\n",
      "Region recall: 0.809971432249787\n"
     ]
    }
   ],
   "source": [
    "# Baselines\n",
    "baseline_beta = np.linalg.solve(X_train.T @ X_train, X_train.T @ Y_train)\n",
    "base_MAE = test_MAE(X_test, Y_test, baseline_beta, B)\n",
    "\n",
    "hg_MAE = test_MAE(X_test, Y_test, beta, hg_R)\n",
    "ho_MAE = test_MAE(X_test, Y_test, beta, ho_R)\n",
    "\n",
    "hg_incl = np.sum(in_box(X_test[:, :-1], hg_R)) / len(X_test)\n",
    "ho_incl = np.sum(in_box(X_test[:, :-1], ho_R)) / len(X_test)\n",
    "\n",
    "if dataset == 'Synthetic':\n",
    "    true_vol = box_intersection(true_R, true_R)\n",
    "\n",
    "    hg_vol = box_intersection(hg_R, hg_R)\n",
    "    hg_intersect_vol = box_intersection(hg_R, true_R)\n",
    "\n",
    "    ho_vol = box_intersection(ho_R, ho_R)\n",
    "    ho_intersect_vol = box_intersection(ho_R, true_R)\n",
    "\n",
    "    hg_prec = hg_intersect_vol / hg_vol\n",
    "    hg_rec  = hg_intersect_vol / true_vol\n",
    "    \n",
    "    ho_prec = ho_intersect_vol / ho_vol\n",
    "    ho_rec  = ho_intersect_vol / true_vol\n",
    "\n",
    "print('Results')\n",
    "print(f'Baseline MAE: {base_MAE}')\n",
    "print('---------------------------------')\n",
    "print('Hard thresholding + growing box:')\n",
    "print(f'MAE: {hg_MAE}')\n",
    "print(f'Incl. frac: {hg_incl}')\n",
    "if dataset == 'Synthetic':\n",
    "    print(f'Region precision: {hg_prec}')\n",
    "    print(f'Region recall: {hg_rec}')\n",
    "print('---------------------------------')\n",
    "print('Hard thresholding + optimization:')\n",
    "print(f'MAE: {ho_MAE}')\n",
    "print(f'Incl. frac: {ho_incl}')\n",
    "if dataset == 'Synthetic':\n",
    "    print(f'Region precision: {ho_prec}')\n",
    "    print(f'Region recall: {ho_rec}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
