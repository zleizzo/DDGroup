{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(x, lb, ub):\n",
    "    p = x.copy()\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if x[i] < lb[i]:\n",
    "            p[i] = lb[i]\n",
    "        elif x[i] > ub[i]:\n",
    "            p[i] = ub[i]\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def in_box(x, lb, ub):\n",
    "    if (x == proj(x, lb, ub)).all():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def fit(X, Y):\n",
    "    n = len(Y)\n",
    "    X_copy = X.copy()\n",
    "    # X_copy = np.append(X_copy, np.ones((n, 1)), axis = 1)\n",
    "\n",
    "    XTX = X_copy.T @ X_copy\n",
    "    beta_hat = np.linalg.solve(XTX, X_copy.T @ Y)\n",
    "    min_eig = min(np.linalg.eigvals(XTX)) / len(X_copy)\n",
    "    print(beta_hat)\n",
    "    return beta_hat, min_eig\n",
    "\n",
    "\n",
    "def cutoff(std, min_eig, n_core, n_full, alpha, x):\n",
    "    d = len(x)\n",
    "    return std * (np.linalg.norm(x) * np.sqrt(d * np.log(4 * d / alpha) / n_core) / min_eig + np.sqrt(2 * np.log(4 * n_full / alpha)))\n",
    "\n",
    "\n",
    "def res(x, y, beta):\n",
    "    # return abs(y - np.dot(beta[:-1], x) - beta[-1])\n",
    "    return abs(y - np.dot(x, beta))\n",
    "\n",
    "\n",
    "def directed_infty_norm(x, S):\n",
    "    best = 0\n",
    "    for j in range(len(x)):\n",
    "        if S[j] != set():\n",
    "            best = max(best, max([x[j] * s for s in S[j]]))\n",
    "    return best\n",
    "\n",
    "\n",
    "def largest_box_heuristic(X, B):\n",
    "    # B is a d x 2 array containing the maximum allowed box.\n",
    "    # B must contain the origin and the origin must be contained in the final selected region.\n",
    "    # Any valid point x must have B[i,0] <= x[i] <= B[i,1].\n",
    "    # All of the points in X should be within this box.\n",
    "    n, d = X.shape\n",
    "    X2 = X.copy()\n",
    "    B2 = B.copy()\n",
    "    S = [set([-1,1]) for j in range(d)]\n",
    "    # print(S)\n",
    "\n",
    "    while X2.any():\n",
    "        directed_infty_norms = [directed_infty_norm(x, S) for x in X2]\n",
    "        i = np.argmin(directed_infty_norms) # i = point which supports the new side\n",
    "        j = list(np.abs(X2[i]) == directed_infty_norms[i]).index(True) # j = dimension which is being supported\n",
    "        sign = int(np.sign(X2[i, j]))\n",
    "\n",
    "        S[j].remove(sign)\n",
    "        B2[j, int((sign + 1)/2)] = X2[i, j]\n",
    "        # print(X2[i,j])\n",
    "        # print(B2[j, int((sign + 1)/2)])\n",
    "\n",
    "        X2 = X2[[k for k in range(len(X2)) if sign * X2[k, j] < directed_infty_norms[i]]]\n",
    "\n",
    "    return B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization-based approach\n",
    "Here we implement the optimization-based approach to region finding. Let R denote (a parameterization of) our estimate for the region. The idea is to maximize $$\\textrm{vol}(R) - \\lambda \\sum_{i=1}^n f_i(R),$$ where $f_i(R)$ are functions which penalize R for including rejected points and potentially reward R for including non-rejected points.\n",
    "\n",
    "Note: We may want to include a normalized volume in the objective. When the region is small, the gradient of the volume will be close to vanishing, but when the region is large, the gradient of the volume will be huge.\n",
    "\n",
    "## Hard-thresholding method\n",
    "We first hard-threshold each training point based on a residual cutoff on the core fit, giving a binary label $r_i \\in \\{0,1\\}$ which is 1 if the i-th point is rejected and 0 if it is not. We then define $$f_i(R) = r_i \\exp(-c_1 d(x_i, R)) - (1-r_i) \\exp(-c_2 d(x_i, R)).$$\n",
    "Here $c_i$ are constants which control how much a rejected point is penalized (resp. a non-rejected point is rewarded) for being close to the approximate region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def vol(R):\n",
    "    v = 1.\n",
    "    for i in range(len(R)):\n",
    "        v *= R[i, 1] - R[i, 0]  # Note: We might run into problems if lb > ub, watch for this.\n",
    "    return v\n",
    "\n",
    "\n",
    "def dist(x, R):\n",
    "    return torch.linalg.norm(x - torch.clamp(x, R[:, 0], R[:, 1]), dim = len(x.shape) - 1)\n",
    "\n",
    "\n",
    "def hard_obj(X, R, excluded, reg, c1 = 1., c2 = 1.):\n",
    "    return vol(R) - reg * torch.sum(excluded * torch.exp(-c1 * dist(X, R)) - (1 - excluded) * torch.exp(-c2 * dist(X, R)))\n",
    "\n",
    "\n",
    "def hard_train(X, init_R, excluded, reg, iters, lr, c1 = 1., c2 = 1.):\n",
    "    R = init_R.clone()\n",
    "    for i in range(iters):\n",
    "        obj = hard_obj(X, R, excluded, reg, c1, c2)\n",
    "        if R.grad is not None:\n",
    "            R.zero_grad_()\n",
    "        obj.backward()\n",
    "        with torch.no_grad():\n",
    "            R += lr * R.grad.data\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data/regr'\n",
    "\n",
    "names_data = ['Dutch_drinking_inh', 'Dutch_drinking_wm', 'Dutch_drinking_sha', 'Brazil_health_heart', \n",
    "              'Brazil_health_stroke', 'Korea_grip', 'China_glucose_women2', 'China_glucose_men2', \n",
    "              'Spain_Hair', 'China_HIV']\n",
    "\n",
    "for name_data in names_data:\n",
    "    X, y, names_covariates = load_regr_data(name_data, dir_data)\n",
    "    y = y.astype(np.float)\n",
    "    print(name_data, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, names_covariates = load_regr_data('Brazil_health_stroke', dir_data)\n",
    "n = len(Y)\n",
    "d = len(X[0])\n",
    "\n",
    "X -= np.mean(X, axis = 0)\n",
    "X = np.append(X, np.ones(n), axis = 1)\n",
    "\n",
    "B = np.column_stack([np.min(X, axis = 0), np.max(X, axis = 0)])\n",
    "\n",
    "test_ind = np.random.sample(range(n))\n",
    "train_ind = [i for i in range(n) if i not in test_ind]\n",
    "\n",
    "X_test = X[test_ind].copy()\n",
    "Y_test = Y[test_ind].copy()\n",
    "\n",
    "X = X[train_ind].copy()\n",
    "Y = Y[train_ind].copy()\n",
    "\n",
    "assert len(Y_test) + len(Y) == n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linalg.solve(X.T @ X, X.T @ Y)\n",
    "res  = np.abs(X @ beta - Y)\n",
    "core_ind = np.argsort(res)[:int(n / 10)]\n",
    "\n",
    "core_X = X[core_ind]\n",
    "core_Y = Y[core_ind]\n",
    "\n",
    "T = 10\n",
    "for i in range(T):\n",
    "    new_beta = np.linalg.solve(core_X.T @ core_X, core_X.T @ core_Y)\n",
    "    if np.linalg.norm(new_beta - beta) < 1e-3:\n",
    "        print(f'Found stable core ({i} iterations)')\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        res = np.abs(X @ new_beta - Y)\n",
    "        new_core_ind = np.argsort(res)[:int(n / 10)]\n",
    "        core_X = X[core_ind]\n",
    "        core_y = Y[core_ind]\n",
    "        beta = new_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_beta, min_eig = fit(core_X, core_Y)\n",
    "\n",
    "s_hat = np.sqrt(np.sum((core_X @ core_beta - core_Y) ** 2) / (len(core_Y) - len(X[0]) - 1))\n",
    "alpha = 0.05\n",
    "\n",
    "\n",
    "excluded = np.zeros(n)\n",
    "for k in range(n):\n",
    "    if res(X[k], y[k], core_beta) > cutoff(s_hat, min_eig, int(n / 10), n, alpha, x):\n",
    "        excluded[k] = 1\n",
    "\n",
    "approx_region = largest_box_heuristic(X[excluded == 1], B)\n",
    "\n",
    "reg = 1.\n",
    "iters = 100\n",
    "lr = 0.1\n",
    "\n",
    "init_R = 0.1 * torch.ones(d, 2)\n",
    "init_R[:, 0] *= -1\n",
    "init_R.requires_grad = True\n",
    "opt_approx_region = hard_train(X, init_R, excluded, reg, iters, lr, c1 = 1., c2 = 1.)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
